Creating RL model...
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Starting RL training for 100000 steps...
step: [92m12 / None[0mm
Traceback (most recent call last):
  File "/media/tao/E8F6F2ECF6F2BA40/bimanial_manipulation/RoboTwin/script/residual_train.py", line 423, in <module>
    model_path = train_rl(usr_args)
  File "/media/tao/E8F6F2ECF6F2BA40/bimanial_manipulation/RoboTwin/script/residual_train.py", line 373, in train_rl
    ppo_model.learn(
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 202, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py", line 313, in forward
    return th.cat(encoded_tensor_list, dim=1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/media/tao/E8F6F2ECF6F2BA40/bimanial_manipulation/RoboTwin/script/residual_train.py", line 423, in <module>
    model_path = train_rl(usr_args)
  File "/media/tao/E8F6F2ECF6F2BA40/bimanial_manipulation/RoboTwin/script/residual_train.py", line 373, in train_rl
    ppo_model.learn(
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 202, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 645, in forward
    features = self.extract_features(obs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 672, in extract_features
    return super().extract_features(obs, self.features_extractor if features_extractor is None else features_extractor)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/policies.py", line 131, in extract_features
    return features_extractor(preprocessed_obs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tao/anaconda3/envs/RoboTwin/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py", line 313, in forward
    return th.cat(encoded_tensor_list, dim=1)
KeyboardInterrupt
